{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25488c18-307b-4fe4-bf8b-d97e02820c3a",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12675b29-8fa6-419c-9dd3-9cc853d908f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, sys, inspect\n",
    "import torchvision as tv\n",
    "import torchvision.models as models\n",
    "import argparse\n",
    "import time\n",
    "from scipy.stats import binom\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from scipy.optimize import brentq\n",
    "from scipy.stats import binom, beta\n",
    "from scipy.special import softmax\n",
    "import pdb\n",
    "import glob\n",
    "import seaborn as sns\n",
    "dirname = str(pathlib.Path().absolute())\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))\n",
    "from matplotlib.font_manager import FontProperties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dc0a0-bb50-4995-b065-66329699c25e",
   "metadata": {},
   "source": [
    "### Helping function for PCOSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4971a9-2590-47f2-a6ff-db7ce566d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisyRC(range_bounds, D, sigma):\n",
    "    \"\"\"\n",
    "    Noisy Range Count for float values with Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    range_bounds (tuple): A tuple (a, b) representing the range [a, b].\n",
    "    D (list): The sorted dataset.\n",
    "    sigma (float): The standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    int: The noisy count of elements in the range [a, b].\n",
    "    \"\"\"\n",
    "    a, b = range_bounds\n",
    "    count = sum(1 for z in D if a <= z <= b)\n",
    "    noise = np.random.normal(0, sigma)\n",
    "    noisy_count = count + noise\n",
    "    return max(0, int(np.floor(noisy_count)))  # Ensure non-negative count\n",
    "\n",
    "def PrivQuant(D, alpha, rho, lower_bound=0, upper_bound=1, delta=1e-10):\n",
    "    \"\"\"\n",
    "    Differentially Private Quantile Approximation Algorithm without integer conversion.\n",
    "\n",
    "    Parameters:\n",
    "    D (list): The sorted dataset.\n",
    "    alpha (float): The quantile level (e.g., 0.5 for median).\n",
    "    rho (float): The privacy parameter (smaller = more private).\n",
    "    lower_bound (float): Lower bound of the search space.\n",
    "    upper_bound (float): Upper bound of the search space.\n",
    "    delta (float): Small positive value to ensure convergence.\n",
    "\n",
    "    Returns:\n",
    "    float: A differentially private approximation of the quantile x_{(m)}.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    n = len(D)\n",
    "    max_iterations = int(np.ceil(np.log2((upper_bound - lower_bound) / delta)))\n",
    "    sigma = np.sqrt(max_iterations / (2 * rho))  # Noise scale for Gaussian mechanism\n",
    "    m = int(np.ceil((1 - alpha) * (n + 1)))\n",
    "\n",
    "    left, right = lower_bound, upper_bound\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        mid = (left + right) / 2\n",
    "        c = NoisyRC((lower_bound, mid), D, sigma)\n",
    "        \n",
    "        if c < m:\n",
    "            left = mid + delta\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    return np.round((left + right) / 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a32eb1-5afd-4f16-940d-65b8e065f30a",
   "metadata": {},
   "source": [
    "### Helping function for EXPONQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c9c174-22ed-4f1b-a41c-e30865630347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_qtilde(n,alpha,gamma,epsilon,m):\n",
    "    qtilde = (n+1)*(1-alpha)/(n*(1-gamma*alpha))+2/(epsilon*n)*np.log(m/(gamma*alpha))\n",
    "    qtilde = min(qtilde, 1-1e-12)\n",
    "    return qtilde\n",
    "\n",
    "def generate_scores(n):\n",
    "    return np.random.uniform(size=(n,))\n",
    "\n",
    "def hist_2_cdf(cumsum, bins, n):\n",
    "    def _cdf(t):\n",
    "        if t > bins[-2]:\n",
    "            return 1.0\n",
    "        elif t < bins[1]:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1-cumsum[np.searchsorted(bins, t)]/n\n",
    "    return _cdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_private_quantile(scores, alpha, epsilon, gamma, bins):\n",
    "    n = scores.shape[0]\n",
    "    epsilon_normed = epsilon*min(alpha, 1-alpha)\n",
    "    # Get the quantile\n",
    "    qtilde = get_qtilde(n, alpha, gamma, epsilon, bins.shape[0])\n",
    "    scores = scores.squeeze()\n",
    "    score_to_bin = np.digitize(scores,bins)\n",
    "    binned_scores = bins[np.minimum(score_to_bin,bins.shape[0]-1)]\n",
    "    w1 = np.digitize(binned_scores, bins)\n",
    "    w2 = np.digitize(binned_scores, bins, right=True)\n",
    "    # Clip bins\n",
    "    w1 = np.maximum(np.minimum(w1,bins.shape[0]-1),0)\n",
    "    w2 = np.maximum(np.minimum(w2,bins.shape[0]-1),0)\n",
    "    lower_mass = np.bincount(w1,minlength=bins.shape[0]).cumsum()/qtilde\n",
    "    upper_mass = (n-np.bincount(w2,minlength=bins.shape[0]).cumsum())/(1-qtilde)\n",
    "    w = np.maximum( lower_mass , upper_mass )\n",
    "    sampling_probabilities = softmax(-(epsilon_normed/2)*w)\n",
    "    # Check\n",
    "    sampling_probabilities = sampling_probabilities/sampling_probabilities.sum()\n",
    "    qhat = np.random.choice(bins,p=sampling_probabilities)\n",
    "    return qhat\n",
    "\n",
    "# Optimal gamma is a root.\n",
    "def get_optimal_gamma(scores,n,alpha,m,epsilon):\n",
    "    a = alpha**2\n",
    "    b = - ( alpha*epsilon*(n+1)*(1-alpha)/2 + 2*alpha )\n",
    "    c = 1\n",
    "    best_q = 1\n",
    "    gamma1 = (-b + np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "    gamma2 = (-b - np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "\n",
    "    gamma1 = min(max(gamma1,1e-12),1-1e-12)\n",
    "    gamma2 = min(max(gamma2,1e-12),1-1e-12)\n",
    "\n",
    "    bins = np.linspace(0,1,m)\n",
    "\n",
    "    q1 = get_private_quantile(scores, alpha, epsilon, gamma1, bins)\n",
    "    q2 = get_private_quantile(scores, alpha, epsilon, gamma2, bins)\n",
    "\n",
    "    return (gamma1, q1) if q1 < q2 else (gamma2, q2)\n",
    "\n",
    "def get_optimal_gamma_m(n, alpha, epsilon):\n",
    "    candidates_m = np.logspace(4,6,50).astype(int)\n",
    "    scores = np.random.rand(n,1)\n",
    "    best_m = int(1/alpha)\n",
    "    best_gamma = 1\n",
    "    best_q = 1\n",
    "    for m in candidates_m:\n",
    "        gamma, q = get_optimal_gamma(scores,n,alpha,m,epsilon)\n",
    "        if q < best_q:\n",
    "            best_q = q\n",
    "            best_m = m\n",
    "            best_gamma = gamma\n",
    "    return best_m, best_gamma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_conformal_scores(scores, labels):\n",
    "    conformal_scores = torch.tensor([scores[i,labels[i]] for i in range(scores.shape[0])]) \n",
    "    return conformal_scores \n",
    "\n",
    "def get_shat_from_scores_private(scores, alpha, epsilon, gamma, score_bins):\n",
    "    shat = get_private_quantile(scores, alpha, epsilon, gamma, score_bins)\n",
    "    return shat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9c2c4-81f9-4a3d-91a6-85f6e4608d2b",
   "metadata": {},
   "source": [
    "### Helping function for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb11eefd-bb24-476f-ae76-630e04fae705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(modelname):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model by name.\n",
    "\n",
    "    Args:\n",
    "        modelname (str): Name of the model (e.g., 'ResNet152').\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Pre-trained model.\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Use the new weights parameter\n",
    "    if modelname == 'ResNet18':\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNet50':\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNet101':\n",
    "        model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNet152':\n",
    "        model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ResNeXt101':\n",
    "        model = models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'VGG16':\n",
    "        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'ShuffleNet':\n",
    "        model = models.shufflenet_v2_x1_0(weights=models.ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'Inception':\n",
    "        model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
    "    elif modelname == 'DenseNet161':\n",
    "        model = models.densenet161(weights=models.DenseNet161_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model {modelname} not supported\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Computes logits and targets from a model and loader\n",
    "def get_logits_targets(model, loader):\n",
    "    \"\"\"\n",
    "    Compute logits and targets for a dataset using a model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pre-trained model.\n",
    "        loader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tuple of (logits, targets).\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logits = torch.zeros((len(loader.dataset), 1000))  # 1000 classes in ImageNet\n",
    "    labels = torch.zeros((len(loader.dataset),))\n",
    "    i = 0\n",
    "\n",
    "    print(f'Computing logits for model (only happens once).')\n",
    "    with torch.no_grad():\n",
    "        for x, targets in tqdm(loader, desc=\"Processing batches\"):\n",
    "            batch_logits = model(x.to(device)).detach().cpu()\n",
    "            logits[i:(i + x.shape[0]), :] = batch_logits\n",
    "            labels[i:(i + x.shape[0])] = targets.cpu()\n",
    "            i += x.shape[0]\n",
    "\n",
    "    # Construct the dataset\n",
    "    dataset_logits = torch.utils.data.TensorDataset(logits, labels.long())\n",
    "    return dataset_logits\n",
    "\n",
    "\n",
    "\n",
    "def get_logits_dataset(modelname, datasetname, datasetpath, cache= dirname + '/.cache/'):\n",
    "    fname = cache + datasetname + '/' + modelname + '.pkl' \n",
    "\n",
    "    # If the file exists, load and return it.\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as handle:\n",
    "            return pkl.load(handle)\n",
    "\n",
    "    # Else we will load our model, run it on the dataset, and save/return the output.\n",
    "    model = get_model(modelname)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std =[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "    \n",
    "    dataset = torchvision.datasets.ImageNet(datasetpath, split='val', transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = 32, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "    # Get the logits and targets\n",
    "    dataset_logits = get_logits_targets(model, loader)\n",
    "\n",
    "    # Save the dataset \n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pkl.dump(dataset_logits, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dataset_logits\n",
    "\n",
    "    \n",
    "def fix_randomness(seed=0):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_imagenet_classes():\n",
    "    df = pd.read_csv(dirname + '/map_clsloc.txt', delimiter=' ')\n",
    "    arr = df['name'].to_numpy()\n",
    "    return arr\n",
    "\n",
    "def get_metrics_precomputed(est_labels,labels,losses,num_classes):\n",
    "    labels = torch.nn.functional.one_hot(labels,num_classes)\n",
    "    empirical_losses = (losses.view(1,-1) * (labels * (1-est_labels))).sum(dim=1)\n",
    "    sizes = est_labels.sum(dim=1)\n",
    "    return empirical_losses, sizes \n",
    "\n",
    "\n",
    "def platt_logits(calib_dataset, max_iters=10, lr=0.01, epsilon=0.01):\n",
    "    calib_loader = torch.utils.data.DataLoader(calib_dataset, batch_size=1024, shuffle=False, pin_memory=True) \n",
    "    nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    T = nn.Parameter(torch.Tensor([1.3]).cuda())\n",
    "\n",
    "    optimizer = optim.SGD([T], lr=lr)\n",
    "    for iter in range(max_iters):\n",
    "        T_old = T.item()\n",
    "        for x, targets in calib_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda()\n",
    "            x.requires_grad = True\n",
    "            out = x/T\n",
    "            loss = nll_criterion(out, targets.long().cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if abs(T_old - T.item()) < epsilon:\n",
    "            break\n",
    "    return T \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a542d1-c1d5-48c8-aff9-e72a3ce07485",
   "metadata": {},
   "source": [
    "### Helping function for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ff4cdb-9128-4612-a025-29c7e0fdf166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_precomputed(conformal_scores, raw_scores, alpha, epsilon, num_calib,score_bins,gamma):\n",
    "        \n",
    "    total=conformal_scores.shape[0]\n",
    "    perm = torch.randperm(conformal_scores.shape[0])\n",
    "    conformal_scores = conformal_scores[perm]\n",
    "    raw_scores = raw_scores[perm]\n",
    "    calib_conformal_scores, val_conformal_scores = (1-conformal_scores[0:num_calib], 1-conformal_scores[num_calib:])\n",
    "    calib_raw_scores, val_raw_scores = (1-raw_scores[0:num_calib], 1-raw_scores[num_calib:])\n",
    "\n",
    "    shat = get_shat_from_scores_private(calib_conformal_scores, alpha, epsilon, gamma, score_bins)\n",
    "    threshold_PrivQuant = PrivQuant(calib_conformal_scores, alpha, rho=epsilon, lower_bound=0, upper_bound=1, delta = 1e-16) \n",
    "   \n",
    "\n",
    "    corrects = (val_conformal_scores) < shat\n",
    "    corrects_PrivQuant = (val_conformal_scores) < threshold_PrivQuant\n",
    "    sizes = ((val_raw_scores) < shat).sum(dim=1)\n",
    "    sizes_PrivQuant = ((val_raw_scores) < threshold_PrivQuant).sum(dim=1)\n",
    "\n",
    "    return corrects.float().mean().item(),corrects_PrivQuant.float().mean().item(),torch.tensor(sizes),torch.tensor(sizes_PrivQuant), shat,threshold_PrivQuant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8eb74-7c47-42f7-9d07-1b1cb5458d24",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763fd9a8-5f58-4378-8508-26885bcd8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_221547/1696106157.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return corrects.float().mean().item(),corrects_PrivQuant.float().mean().item(),torch.tensor(sizes),torch.tensor(sizes_PrivQuant), shat,threshold_PrivQuant\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:25<00:00, 12.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_221547/1696106157.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return corrects.float().mean().item(),corrects_PrivQuant.float().mean().item(),torch.tensor(sizes),torch.tensor(sizes_PrivQuant), shat,threshold_PrivQuant\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:25<00:00, 12.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_221547/1696106157.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return corrects.float().mean().item(),corrects_PrivQuant.float().mean().item(),torch.tensor(sizes),torch.tensor(sizes_PrivQuant), shat,threshold_PrivQuant\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:25<00:00, 12.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_221547/1696106157.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return corrects.float().mean().item(),corrects_PrivQuant.float().mean().item(),torch.tensor(sizes),torch.tensor(sizes_PrivQuant), shat,threshold_PrivQuant\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:25<00:00, 12.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_221547/1696106157.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return corrects.float().mean().item(),corrects_PrivQuant.float().mean().item(),torch.tensor(sizes),torch.tensor(sizes_PrivQuant), shat,threshold_PrivQuant\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:25<00:00, 12.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_221547/1696106157.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return corrects.float().mean().item(),corrects_PrivQuant.float().mean().item(),torch.tensor(sizes),torch.tensor(sizes_PrivQuant), shat,threshold_PrivQuant\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:26<00:00, 13.01s/it]\n"
     ]
    }
   ],
   "source": [
    "def experiment(alpha, epsilons, num_calib, batch_size, imagenet_val_dir):\n",
    "    # Clear cache files\n",
    "    cache_files = glob.glob(f'.cache/opt_{alpha}_*_dataframe_trial.pkl')\n",
    "    for cache_file in cache_files:\n",
    "        os.remove(cache_file)\n",
    "    \n",
    "    df_list = []\n",
    "    for epsilon in epsilons:\n",
    "        # Create the .cache directory if it doesn't exist\n",
    "        os.makedirs('.cache', exist_ok=True)\n",
    "        \n",
    "        fname = f'.cache/opt_{alpha}_{epsilon}_{num_calib}_dataframe_trial.pkl'\n",
    "        mstar, gammastar = get_optimal_gamma_m(num_calib, alpha, epsilon)\n",
    "        m = mstar\n",
    "        gamma = gammastar\n",
    "        score_bins = np.linspace(0, 1, m)\n",
    "    \n",
    "        # Define the expected columns\n",
    "        expected_columns = [\"$\\\\hat{s}$\", \"$\\\\hat{q}_$PCOQS\", \"EXPONQ\", \"PCOQS\", \"sizes_EXPONQ\", \"sizes_PCOQS\", \"$\\\\alpha$\", \"$\\\\epsilon$\"]\n",
    "    \n",
    "        try:\n",
    "            # Try to load the cached DataFrame\n",
    "            df = pd.read_pickle(fname)\n",
    "        except FileNotFoundError:\n",
    "            # If the cached file doesn't exist, compute the DataFrame\n",
    "            dataset_precomputed = get_logits_dataset('ResNet152', 'Imagenet', imagenet_val_dir)\n",
    "            print('Dataset loaded')\n",
    "        \n",
    "            classes_array = get_imagenet_classes()\n",
    "            T = platt_logits(dataset_precomputed)\n",
    "        \n",
    "            logits, labels = dataset_precomputed.tensors\n",
    "            scores = (logits / T.cpu()).softmax(dim=1)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                conformal_scores = get_conformal_scores(scores, labels)\n",
    "                local_df_list = []\n",
    "                for i in tqdm(range(num_trials)):\n",
    "                    cvg1, cvg2, szs1, szs2, shat, threshold_PrivQuant = trial_precomputed(conformal_scores, scores, alpha, epsilon, num_calib,score_bins,gamma)\n",
    "                    dict_local = {\n",
    "                        \"$\\\\hat{s}$\": shat,\n",
    "                        \"$\\\\hat{q}_$PCOQS\": threshold_PrivQuant,\n",
    "                        \"EXPONQ\": cvg1,\n",
    "                        \"PCOQS\": cvg2,\n",
    "                        \"sizes_EXPONQ\": [szs1],\n",
    "                        \"sizes_PCOQS\": [szs2],\n",
    "                        \"$\\\\alpha$\": alpha,\n",
    "                        \"$\\\\epsilon$\": epsilon\n",
    "                    }\n",
    "                    df_local = pd.DataFrame(dict_local)\n",
    "                    local_df_list.append(df_local)\n",
    "        \n",
    "                # Combine all local DataFrames into one\n",
    "                df = pd.concat(local_df_list, axis=0, ignore_index=True)\n",
    "        \n",
    "                # Ensure the DataFrame has the expected columns\n",
    "                df = df.reindex(columns=expected_columns)\n",
    "        \n",
    "                \n",
    "            df_list.append(df)\n",
    "\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sns.set(palette='pastel',font='serif')\n",
    "    sns.set_style('white')\n",
    "    fix_randomness(seed=0)\n",
    "\n",
    "    imagenet_val_dir = '/home/ogonna/Differential_Privacy_Project/Conformal_Prediction_Project/ImageNet_Data_Simulation/' # TODO: put your imagenet directory here\n",
    "\n",
    "    alpha = 0.1\n",
    "    epsilons = [0.05,0.1,0.5,1,3,5]\n",
    "    num_calib = 30000 \n",
    "    num_trials = 100\n",
    "\n",
    "    results=experiment(alpha, epsilons, num_calib, batch_size=128, imagenet_val_dir=imagenet_val_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85461420-449c-46ba-8376-6402a38e10bb",
   "metadata": {},
   "source": [
    "### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d7c525-02fb-4203-ba61-fab6b3c70dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'df_list_ImageNet_results_trial.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "            pkl.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612a258-0717-451c-b6e2-b00a9ce11fd9",
   "metadata": {},
   "source": [
    "### Processing and saving result for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b86659-8f09-4825-9686-8c20ea03d981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for EXPONQ at epsilon=0.05\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for PCOQS at epsilon=0.05\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for EXPONQ at epsilon=0.1\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for PCOQS at epsilon=0.1\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for EXPONQ at epsilon=0.5\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for PCOQS at epsilon=0.5\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for EXPONQ at epsilon=1\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for PCOQS at epsilon=1\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for EXPONQ at epsilon=3\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for PCOQS at epsilon=3\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for EXPONQ at epsilon=5\n",
      "Warning: Expected 2000000 size points, got 40000\n",
      "Warning: No size averages for PCOQS at epsilon=5\n",
      "Processing complete. Files saved:\n",
      "- coverage_epsilon_[epsilon].csv\n",
      "- avg_size_epsilon_[epsilon].csv\n"
     ]
    }
   ],
   "source": [
    "def compute_trial_averages(size_series, trials=100, eval_points=20000):\n",
    "    \"\"\"\n",
    "    Compute average set size per trial from size series\n",
    "    Returns: Array of 100 average sizes (one per trial)\n",
    "    \"\"\"\n",
    "    # First flatten and convert all values to floats\n",
    "    sizes = []\n",
    "    for val in size_series.explode().dropna():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            sizes.append(float(val.item()))\n",
    "        else:\n",
    "            sizes.append(float(val))\n",
    "    \n",
    "    if len(sizes) != trials * eval_points:\n",
    "        print(f\"Warning: Expected {trials*eval_points} size points, got {len(sizes)}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    # Reshape to (trials, eval_points) and compute trial averages\n",
    "    size_array = np.array(sizes).reshape(trials, eval_points)\n",
    "    return np.mean(size_array, axis=1)\n",
    "\n",
    "def safe_to_dataframe(data_dict):\n",
    "    \"\"\"Convert dictionary to DataFrame, handling unequal lengths\"\"\"\n",
    "    if not data_dict:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    max_len = max(len(v) for v in data_dict.values())\n",
    "    padded = {k: np.pad(v, (0, max_len - len(v)), \n",
    "             mode='constant', constant_values=np.nan)\n",
    "             for k, v in data_dict.items()}\n",
    "    return pd.DataFrame(padded)\n",
    "\n",
    "def main():\n",
    "    # Load your data\n",
    "    try:\n",
    "        with open('df_list_ImageNet_results_trial.pkl', 'rb') as f:\n",
    "            df_list = pkl.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found\")\n",
    "        return\n",
    "    except pkl.PickleError:\n",
    "        print(\"Error: Could not unpickle the file\")\n",
    "        return\n",
    "\n",
    "    # Epsilon values\n",
    "    epsilons = [0.05, 0.1, 0.5, 1, 3, 5]\n",
    "\n",
    "    # Initialize storage\n",
    "    results = {\n",
    "        'coverage': {epsilon: {} for epsilon in epsilons},\n",
    "        'avg_size': {epsilon: {} for epsilon in epsilons}  \n",
    "    }\n",
    "\n",
    "    # Process each epsilon's DataFrame\n",
    "    for epsilon, df in zip(epsilons, df_list):\n",
    "        try:\n",
    "            # Coverage data\n",
    "            for method in [\"EXPONQ\", \"PCOQS\"]:\n",
    "                if method in df.columns:\n",
    "                    cov_data = df[method].dropna().astype(float).values\n",
    "                    results['coverage'][epsilon][method] = cov_data\n",
    "            \n",
    "            # Size data - compute trial averages\n",
    "            for method in [\"EXPONQ\", \"PCOQS\"]:\n",
    "                size_key = f\"sizes_{method}\"\n",
    "                if size_key in df.columns:\n",
    "                    trial_avgs = compute_trial_averages(df[size_key])\n",
    "                    if len(trial_avgs) > 0:\n",
    "                        results['avg_size'][epsilon][method] = trial_avgs\n",
    "                    else:\n",
    "                        print(f\"Warning: No size averages for {method} at epsilon={epsilon}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing epsilon={epsilon}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save coverage data\n",
    "    for ε in epsilons:\n",
    "        if results['coverage'][epsilon]:\n",
    "            df = safe_to_dataframe(results['coverage'][epsilon])\n",
    "            if not df.empty:\n",
    "                df.to_csv(f'coverage_epsilon_{epsilon}.csv', index=False)\n",
    "\n",
    "    # Save average size data (one file per epsilon)\n",
    "    for ε in epsilons:\n",
    "        if results['avg_size'][epsilon]:\n",
    "            df = safe_to_dataframe(results['avg_size'][epsilon])\n",
    "            if not df.empty:\n",
    "                df.to_csv(f'avg_size_epsilon_{epsilon}.csv', index=False)\n",
    "            else:\n",
    "                print(f\"No average size data for epsilon={epsilon}\")\n",
    "\n",
    "    print(\"Processing complete. Files saved:\")\n",
    "    print(\"- coverage_epsilon_[epsilon].csv\")\n",
    "    print(\"- avg_size_epsilon_[epsilon].csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c968fc-14e1-42e8-9839-275d89119fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbc9e8-028b-4ded-94a0-81a4ca72882a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
