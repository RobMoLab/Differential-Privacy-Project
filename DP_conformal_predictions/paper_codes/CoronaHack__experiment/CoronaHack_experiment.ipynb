{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eabffe6-4ab2-4ae7-8cc0-446dc701fc55",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed80ba4-9b76-43e3-b977-38da42b67c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../'))\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pathlib\n",
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import shutil\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9ca3d-da15-4529-a7ef-d2ea938e4a25",
   "metadata": {},
   "source": [
    "### NoisyRC function and PrivQuant algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa13ec-8690-4cd6-819b-68cfdaeb4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisyRC(range_bounds, D, sigma):\n",
    "    \"\"\"\n",
    "    Noisy Range Count for float values with Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    range_bounds (tuple): A tuple (a, b) representing the range [a, b].\n",
    "    D (list): The sorted dataset.\n",
    "    sigma (float): The standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    int: The noisy count of elements in the range [a, b].\n",
    "    \"\"\"\n",
    "    a, b = range_bounds\n",
    "    count = sum(1 for z in D if a <= z <= b)\n",
    "    noise = np.random.normal(0, sigma)\n",
    "    noisy_count = count + noise\n",
    "    return max(0, int(np.floor(noisy_count)))  # Ensure non-negative count\n",
    "\n",
    "def PrivQuant(D, alpha, rho, lower_bound=0, upper_bound=1, delta=1e-10):\n",
    "    \"\"\"\n",
    "    Differentially Private Quantile Approximation Algorithm without integer conversion.\n",
    "\n",
    "    Parameters:\n",
    "    D (list): The sorted dataset.\n",
    "    alpha (float): The quantile level (e.g., 0.5 for median).\n",
    "    rho (float): The privacy parameter (smaller = more private).\n",
    "    lower_bound (float): Lower bound of the search space.\n",
    "    upper_bound (float): Upper bound of the search space.\n",
    "    delta (float): Small positive value to ensure convergence.\n",
    "\n",
    "    Returns:\n",
    "    float: A differentially private approximation of the quantile x_{(m)}.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    n = len(D)\n",
    "    max_iterations = int(np.ceil(np.log2((upper_bound - lower_bound) / delta)))\n",
    "    sigma = np.sqrt(max_iterations / (2 * rho))  # Noise scale for Gaussian mechanism\n",
    "    m = int(np.ceil((1 - alpha) * (n + 1)))\n",
    "\n",
    "    left, right = lower_bound, upper_bound\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        mid = (left + right) / 2\n",
    "        c = NoisyRC((lower_bound, mid), D, sigma)\n",
    "        \n",
    "        if c < m:\n",
    "            left = mid + delta\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    return np.round((left + right) / 2, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e9311-5afe-46f2-940b-3889946cc6ed",
   "metadata": {},
   "source": [
    "### Helping functions for EXPONQ and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe6dad-66ea-4bee-a608-9aa8d55945e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = str(pathlib.Path().absolute())\n",
    "\n",
    "\n",
    "def get_qtilde(n,alpha,gamma,epsilon,m):\n",
    "    qtilde = (n+1)*(1-alpha)/(n*(1-gamma*alpha))+2/(epsilon*n)*np.log(m/(gamma*alpha))\n",
    "    qtilde = min(qtilde, 1-1e-12)\n",
    "    return qtilde\n",
    "\n",
    "def generate_scores(n):\n",
    "    return np.random.uniform(size=(n,))\n",
    "\n",
    "def hist_2_cdf(cumsum, bins, n):\n",
    "    def _cdf(t):\n",
    "        if t > bins[-2]:\n",
    "            return 1.0\n",
    "        elif t < bins[1]:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1-cumsum[np.searchsorted(bins, t)]/n\n",
    "    return _cdf\n",
    "\n",
    "def get_private_quantile(scores, alpha, epsilon, gamma, bins):\n",
    "    n = scores.shape[0]\n",
    "    epsilon_normed = epsilon*min(alpha, 1-alpha)\n",
    "    # Get the quantile\n",
    "    qtilde = get_qtilde(n, alpha, gamma, epsilon, bins.shape[0])\n",
    "    scores = scores.squeeze()\n",
    "    score_to_bin = np.digitize(scores,bins)\n",
    "    binned_scores = bins[np.minimum(score_to_bin,bins.shape[0]-1)]\n",
    "    w1 = np.digitize(binned_scores, bins)\n",
    "    w2 = np.digitize(binned_scores, bins, right=True)\n",
    "    # Clip bins\n",
    "    w1 = np.maximum(np.minimum(w1,bins.shape[0]-1),0)\n",
    "    w2 = np.maximum(np.minimum(w2,bins.shape[0]-1),0)\n",
    "    lower_mass = np.bincount(w1,minlength=bins.shape[0]).cumsum()/qtilde\n",
    "    upper_mass = (n-np.bincount(w2,minlength=bins.shape[0]).cumsum())/(1-qtilde)\n",
    "    w = np.maximum( lower_mass , upper_mass )\n",
    "    sampling_probabilities = softmax(-(epsilon_normed/2)*w)\n",
    "    # Check\n",
    "    sampling_probabilities = sampling_probabilities/sampling_probabilities.sum()\n",
    "    qhat = np.random.choice(bins,p=sampling_probabilities)\n",
    "    return qhat\n",
    "\n",
    "# Optimal gamma is a root.\n",
    "def get_optimal_gamma(scores,n,alpha,m,epsilon):\n",
    "    a = alpha**2\n",
    "    b = - ( alpha*epsilon*(n+1)*(1-alpha)/2 + 2*alpha )\n",
    "    c = 1\n",
    "    best_q = 1\n",
    "    gamma1 = (-b + np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "    gamma2 = (-b - np.sqrt(b**2 - 4*a*c))/(2*a)\n",
    "\n",
    "    gamma1 = min(max(gamma1,1e-12),1-1e-12)\n",
    "    gamma2 = min(max(gamma2,1e-12),1-1e-12)\n",
    "\n",
    "    bins = np.linspace(0,1,m)\n",
    "\n",
    "    q1 = get_private_quantile(scores, alpha, epsilon, gamma1, bins)\n",
    "    q2 = get_private_quantile(scores, alpha, epsilon, gamma2, bins)\n",
    "\n",
    "    return (gamma1, q1) if q1 < q2 else (gamma2, q2)\n",
    "\n",
    "def get_optimal_gamma_m(n, alpha, epsilon):\n",
    "    candidates_m = np.logspace(4,6,50).astype(int)\n",
    "    scores = np.random.rand(n,1)\n",
    "    best_m = int(1/alpha)\n",
    "    best_gamma = 1\n",
    "    best_q = 1\n",
    "    for m in candidates_m:\n",
    "        gamma, q = get_optimal_gamma(scores,n,alpha,m,epsilon)\n",
    "        if q < best_q:\n",
    "            best_q = q\n",
    "            best_m = m\n",
    "            best_gamma = gamma\n",
    "    return best_m, best_gamma\n",
    "\n",
    "\n",
    "\n",
    "def get_conformal_scores(scores, labels):\n",
    "    conformal_scores = torch.tensor([scores[i,labels[i]] for i in range(scores.shape[0])]) \n",
    "    return conformal_scores \n",
    "\n",
    "def get_shat_from_scores_private(scores, alpha, epsilon, gamma, score_bins):\n",
    "    shat = get_private_quantile(scores, alpha, epsilon, gamma, score_bins)\n",
    "    return shat \n",
    "\n",
    "\n",
    "def get_shat_from_scores(scores, alpha):\n",
    "    return np.quantile(scores,1-alpha)\n",
    "\n",
    "\n",
    "def get_model(private, feature_extract=True, cache= dirname + '/.cache/'):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = models.resnet18(pretrained=True).to(device)\n",
    "    set_parameter_requires_grad(model, feature_extract)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "    if private:\n",
    "        model_path = \"\"  #TO DO: Put path to private model here\n",
    "    else:\n",
    "        model_path = \"\"  #TO DO: Put path to non-private model here\n",
    "\n",
    "\n",
    "    # Load the model state dict\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Remove the \"_module.\" prefix from keys if present\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_module.\"):\n",
    "            name = k[8:]  # Remove \"_module.\" prefix\n",
    "        else:\n",
    "            name = k\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load the modified state dict\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computes logits and targets from a model and loader\n",
    "def get_logits_targets(model, loader):\n",
    "    logits = torch.zeros((len(loader.dataset), 3)) # 3 classes in XRAY.\n",
    "    labels = torch.zeros((len(loader.dataset),))\n",
    "    i = 0\n",
    "    print(f'Computing logits for model (only happens once).')\n",
    "    with torch.no_grad():\n",
    "        for x, targets in tqdm(loader):\n",
    "            batch_logits = model(x.cuda()).detach().cpu()\n",
    "            logits[i:(i+x.shape[0]), :] = batch_logits\n",
    "            labels[i:(i+x.shape[0])] = targets.cpu()\n",
    "            i = i + x.shape[0]\n",
    "    \n",
    "    # Construct the dataset\n",
    "    dataset_logits = torch.utils.data.TensorDataset(logits, labels.long()) \n",
    "    return dataset_logits\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_shuffle_split(datasetpath, num_calib, num_val, seed):\n",
    "    # Create training and validation datasets\n",
    "    input_size = 224\n",
    "    batch_size = 256\n",
    "\n",
    "    # Data augmentation and normalization for training\n",
    "    # Just normalization for validation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    fix_randomness(seed)\n",
    "    image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(datasetpath, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "    temp = torch.utils.data.ConcatDataset([image_datasets['train'],image_datasets['val']])\n",
    "    image_datasets['train'], image_datasets['val'] = torch.utils.data.random_split(temp,[len(temp)-num_calib-num_val,num_calib+num_val])\n",
    "    return image_datasets\n",
    "    \n",
    "\n",
    "def get_logits_dataset(private, datasetname, datasetpath, num_calib, num_val, seed, cache= dirname + '/.cache/'):\n",
    "    fname = cache + datasetname + '/' + 'private' + '.pkl'  if private else cache + datasetname + '/nonprivate.pkl'\n",
    "    batch_size = 256\n",
    "\n",
    "    image_datasets = get_dataset_shuffle_split(datasetpath, num_calib, num_val, seed)\n",
    "    # If the file exists, load and return it.\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as handle:\n",
    "            return pickle.load(handle), image_datasets\n",
    "\n",
    "    # Else we will load our model, run it on the dataset, and save/return the output.\n",
    "    model = get_model(private, True)\n",
    "\n",
    "    # get the datasets and loaders\n",
    "    image_datasets = get_dataset_shuffle_split(datasetpath, num_calib, num_val, seed)\n",
    "\n",
    "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "    # Get the logits and targets\n",
    "    dataset_logits_dict = {x: get_logits_targets(model, dataloaders_dict[x]) for x in ['train','val']}\n",
    "\n",
    "    # Save the dataset \n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(dataset_logits_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return dataset_logits_dict, image_datasets\n",
    "\n",
    "def fix_randomness(seed=0):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_metrics_precomputed(est_labels,labels,losses,num_classes):\n",
    "    labels = torch.nn.functional.one_hot(labels,num_classes)\n",
    "    empirical_losses = (losses.view(1,-1) * (labels * (1-est_labels))).sum(dim=1)\n",
    "    sizes = est_labels.sum(dim=1)\n",
    "    return empirical_losses, sizes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6621e70-26e0-48de-813a-4e9f06f30866",
   "metadata": {},
   "source": [
    "### Helping function for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aeab9-7a66-4b5b-8f2c-539d08a960cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=\"checkpoint.tar\", private=False):\n",
    "    root = f'./.cache/'\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    root = root + 'private' if private else root + 'nonprivate'\n",
    "    torch.save(state, root+filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(root+filename, root+\"model_best.pth.tar\")\n",
    "\n",
    "def fix_randomness(seed):\n",
    "    ### Fix randomness \n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def platt_logits(calib_dataset, max_iters=10, lr=0.01, epsilon=0.01):\n",
    "    calib_loader = torch.utils.data.DataLoader(calib_dataset, batch_size=1024, shuffle=False, pin_memory=True) \n",
    "    nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    T = nn.Parameter(torch.Tensor([1.3]).cuda())\n",
    "\n",
    "    optimizer = optim.SGD([T], lr=lr)\n",
    "    for iter in range(max_iters):\n",
    "        T_old = T.item()\n",
    "        for x, targets in calib_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda()\n",
    "            x.requires_grad = True\n",
    "            out = x/T\n",
    "            loss = nll_criterion(out, targets.long().cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if abs(T_old - T.item()) < epsilon:\n",
    "            break\n",
    "    return T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffed7d-c699-4833-a089-fa49afc739a5",
   "metadata": {},
   "source": [
    "### Helping function for comformal prediction experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556e9d2-112a-4319-938d-0108c58fb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_precomputed(conformal_scores, raw_scores, alpha, epsilon, gamma, score_bins, num_calib, privateconformal):\n",
    "    total = conformal_scores.shape[0]\n",
    "    perm = torch.randperm(conformal_scores.shape[0])\n",
    "    conformal_scores = conformal_scores[perm]\n",
    "    raw_scores = raw_scores[perm]\n",
    "    calib_conformal_scores, val_conformal_scores = (1-conformal_scores[0:num_calib], 1-conformal_scores[num_calib:])\n",
    "    calib_raw_scores, val_raw_scores = (1-raw_scores[0:num_calib], 1-raw_scores[num_calib:])\n",
    "\n",
    "    # Always compute non-private results\n",
    "    threshold_nonpriv = get_shat_from_scores(calib_conformal_scores, alpha)\n",
    "    corrects_nonpriv = (val_conformal_scores < threshold_nonpriv)\n",
    "    sizes_nonpriv = (val_raw_scores < threshold_nonpriv).sum(dim=1)\n",
    "    \n",
    "    if privateconformal:\n",
    "        # Only compute private results if privateconformal is True\n",
    "        shat = get_shat_from_scores_private(calib_conformal_scores, alpha, epsilon, gamma, score_bins)\n",
    "        threshold_PrivQuant = PrivQuant(calib_conformal_scores, alpha, rho=epsilon, lower_bound=0, upper_bound=1, delta=1e-10) \n",
    "        \n",
    "        corrects = (val_conformal_scores < shat)\n",
    "        corrects_PrivQuant = (val_conformal_scores < threshold_PrivQuant)\n",
    "        sizes = (val_raw_scores < shat).sum(dim=1)\n",
    "        sizes_PrivQuant = (val_raw_scores < threshold_PrivQuant).sum(dim=1)\n",
    "        \n",
    "        return (\n",
    "            corrects.float().mean().item(),\n",
    "            corrects_PrivQuant.float().mean().item(),\n",
    "            np.nan,  # corrects_nonpriv\n",
    "            sizes,\n",
    "            sizes_PrivQuant,\n",
    "            torch.tensor([]),  #  sizes_nonpriv\n",
    "            shat.item(),\n",
    "            threshold_PrivQuant.item(),\n",
    "            np.nan,  # threshold_nonpriv\n",
    "        )\n",
    "    else:\n",
    "        # When privateconformal is False, only return non-private results\n",
    "        return (\n",
    "            np.nan,  # corrects\n",
    "            np.nan,  # corrects_PrivQuant\n",
    "            corrects_nonpriv.float().mean().item(),  # corrects_nonpriv\n",
    "            torch.tensor([]),  # sizes\n",
    "            torch.tensor([]),  # sizes_PrivQuant\n",
    "            sizes_nonpriv,  # sizes_nonpriv\n",
    "            np.nan,  # shat\n",
    "            np.nan,  # threshold_PrivQuant\n",
    "            threshold_nonpriv.item()  # threshold_nonpriv\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91cb7c-1aa0-4432-88d3-fdc273788b2f",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613be945-a8fa-4bc5-b112-cbd45caf0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, is_private=False, privacy_engine=None):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Define paths for saving the best models\n",
    "    if is_private:\n",
    "        model_path = \".../best_model_private.pth\" #TO DO: Put complete path where you want to save the model best_model_private.pth\n",
    "    else:\n",
    "        model_path = \".../best_model_nonprivate.pth\" #TO DO: Put complete path where you want to save the model best_model_nonprivate.pth\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        if is_private:\n",
    "                            # For private models, use the privacy engine's step\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()  \n",
    "                        else:\n",
    "                            # For non-private models, use standard step\n",
    "                            optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # Save the best model weights\n",
    "                torch.save(best_model_wts, model_path)\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Top level data directory\n",
    "    data_dir = \".../covid_chest_xray/data/imagefolder\"  #TO DO: Put the directory where you saved the CoronaHack dataset\n",
    "\n",
    "    EPSILON =1\n",
    "    DELTA = 1e-5\n",
    "    MAX_GRAD_NORM = 2  # Maximum gradient norm for clipping\n",
    "    num_classes = 3  # Number of classes in the dataset\n",
    "\n",
    "    # Batch size for training\n",
    "    batch_size = 8\n",
    "\n",
    "    # Number of epochs to train for\n",
    "    num_epochs = 15\n",
    "\n",
    "    # Flag for feature extracting\n",
    "    feature_extract = True\n",
    "\n",
    "    # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Create training and validation dataloaders\n",
    "    image_datasets = get_dataset_shuffle_split(data_dir, num_calib=1000, num_val=500, seed=0)\n",
    "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "    # Detect if we have a GPU available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Send the model to GPU\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated\n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name, param in model_ft.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    else:\n",
    "        for name, param in model_ft.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(\"\\t\", name)\n",
    "\n",
    "    # Setup the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Non-private model training\n",
    "    print(\"Training non-private model...\")\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=False, is_private=False)\n",
    "\n",
    "    # Private model training\n",
    "    print(\"Training private model...\")\n",
    "    model_ft_private = models.resnet18(pretrained=True)\n",
    "    set_parameter_requires_grad(model_ft_private, feature_extract)\n",
    "    model_ft_private.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    model_ft_private = model_ft_private.to(device)\n",
    "\n",
    "    optimizer_ft_private = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Add differential privacy using Opacus\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    model_ft_private, optimizer_ft_private, dataloaders_dict['train'] = privacy_engine.make_private_with_epsilon(\n",
    "    module=model_ft_private,\n",
    "    optimizer=optim.SGD(model_ft_private.parameters(), lr=0.001, momentum=0.9),\n",
    "    data_loader=dataloaders_dict['train'],\n",
    "    epochs=num_epochs,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,)\n",
    "\n",
    "\n",
    "    model_ft_private, hist_private = train_model(model_ft_private, dataloaders_dict, criterion, optimizer_ft_private, num_epochs=num_epochs, is_inception=False, is_private=True, privacy_engine=privacy_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30f76b-6615-4867-8dd1-342992b39e08",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e1ba3-d167-4b52-9fa0-1b4c6907ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(alpha, epsilon, num_calib, num_val, datasetpath, privatemodel, privateconformal):\n",
    "    mstar, gammastar = get_optimal_gamma_m(num_calib, alpha, epsilon)\n",
    "    score_bins = np.linspace(0, 1, mstar)\n",
    "    fname = f'.cache/opt_{alpha}_{epsilon}_{num_calib}_{mstar}bins_pm_{privatemodel}_pc_{privateconformal}_dataframe.pkl'\n",
    "    \n",
    "\n",
    "\n",
    "    # Define the expected columns\n",
    "    expected_columns = [\"$\\\\hat{s}$\", \"$\\\\hat{q}_$PrivQuant\", \"threshold_nonpriv\", \"Anas et. al\", \"PrivQuant\", \"NonprivQuant\", \"sizes_Anas et. al\", \"sizes_PrivQuant\",\"sizes_NonprivQuant\", \"$\\\\alpha$\", \"$\\\\epsilon$\"]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_pickle(fname)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    all_data, image_dataset = get_logits_dataset(privatemodel, 'xray', datasetpath, num_calib, num_val, seed=0, cache=dirname + '/.cache/')\n",
    "    print('Dataset loaded')\n",
    "    dataset_precomputed = all_data['val']\n",
    "\n",
    "    classes_array = ['bacterial pneumonia', 'normal', 'viral pneumonia']\n",
    "    T = platt_logits(dataset_precomputed)\n",
    "\n",
    "    logits, labels = dataset_precomputed.tensors\n",
    "    scores = (logits / T.cpu()).softmax(dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        conformal_scores = get_conformal_scores(scores, labels)\n",
    "        local_df_list = []\n",
    "        for i in tqdm(range(num_trials)):\n",
    "            cvg1, cvg2, cvg3, szs1, szs2, szs3, shat, threshold_PrivQuant, threshold_nonpriv = trial_precomputed(conformal_scores, scores, alpha, epsilon, gammastar, score_bins, num_calib, privateconformal)\n",
    "            dict_local = {\n",
    "                \"NonprivQuant\": cvg3,\n",
    "                \"sizes_NonprivQuant\": [szs3],\n",
    "                \"Anas et. al\": cvg1 if privateconformal else np.nan,\n",
    "                \"PrivQuant\": cvg2 if privateconformal else np.nan,\n",
    "                \"sizes_Anas et. al\": [szs1] if privateconformal and szs1 is not None else [torch.tensor([])],\n",
    "                \"sizes_PrivQuant\": [szs2] if privateconformal and szs2 is not None else [torch.tensor([])],\n",
    "                \"$\\\\hat{s}$\": shat if privateconformal else np.nan,\n",
    "                \"$\\\\hat{q}_$PrivQuant\": threshold_PrivQuant if privateconformal else np.nan,\n",
    "                \"threshold_nonpriv\": threshold_nonpriv,\n",
    "                \"$\\\\alpha$\": alpha,\n",
    "                \"$\\\\epsilon$\":epsilon,\n",
    "                \"PrivateConformal\": privateconformal,  \n",
    "                \"PrivateModel\": privatemodel          \n",
    "    }\n",
    "            df_local = pd.DataFrame(dict_local)\n",
    "            local_df_list.append(df_local)\n",
    "\n",
    "        # Combine all local DataFrames into one\n",
    "        df = pd.concat(local_df_list, axis=0, ignore_index=True)\n",
    "\n",
    "        os.makedirs('.cache', exist_ok=True)\n",
    "        df.to_pickle(fname)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shutil.rmtree('.cache', ignore_errors=True)\n",
    "if __name__ == \"__main__\":\n",
    "    sns.set(palette='pastel', font='serif')\n",
    "    sns.set_style('white')\n",
    "    fix_randomness(seed=0)\n",
    "\n",
    "    datasetpath = '.../covid_chest_xray/data/imagefolder'  #TO DO: Put complete path to dataset here\n",
    "    privateconformals = [False, True]\n",
    "    privatemodels = [False, True]\n",
    "\n",
    "    alpha = 0.1\n",
    "    epsilon = 1    # conformal prediction privacy budget\n",
    "    num_calib = 1000\n",
    "    num_val = 500\n",
    "    num_trials = 1000\n",
    "\n",
    "    save_path = 'df_list_Corona_Hack.pkl'       # Saving result\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            df_list = pickle.load(f)\n",
    "    else:\n",
    "        df_list = []\n",
    "        for privateconformal in privateconformals:\n",
    "            for privatemodel in privatemodels:\n",
    "                df_list.append(\n",
    "                    experiment(alpha, epsilon, num_calib, num_val,\n",
    "                               datasetpath=datasetpath,\n",
    "                               privatemodel=privatemodel,\n",
    "                               privateconformal=privateconformal)\n",
    "                )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59fa55-fb0a-481d-95af-c13de9033ecd",
   "metadata": {},
   "source": [
    "### Processing data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932f5af-d773-4422-b855-172d00ebfcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_size_proportions(size_series, trials=1000, eval_points=500):\n",
    "    \"\"\"\n",
    "    Compute proportions of set sizes (1, 2, 3) per trial\n",
    "    Returns: Dict of arrays with proportions for each size\n",
    "    \"\"\"\n",
    "    sizes = []\n",
    "    for val in size_series.explode().dropna():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            sizes.append(float(val.item()))\n",
    "        else:\n",
    "            sizes.append(float(val))\n",
    "    \n",
    "    if len(sizes) != trials * eval_points:\n",
    "        print(f\"Warning: Expected {trials*eval_points} size points, got {len(sizes)}\")\n",
    "        return {'size1': np.array([]), 'size2': np.array([]), 'size3': np.array([])}\n",
    "    \n",
    "    size_array = np.array(sizes).reshape(trials, eval_points)\n",
    "    \n",
    "    return {\n",
    "        'size1': np.mean(size_array == 1, axis=1),\n",
    "        'size2': np.mean(size_array == 2, axis=1),\n",
    "        'size3': np.mean(size_array == 3, axis=1)\n",
    "    }\n",
    "\n",
    "def save_size_proportions_to_csv(data_dict, filename):\n",
    "    \"\"\"Save size proportions with superheader structure in CSV\"\"\"\n",
    "    if not data_dict:\n",
    "        return\n",
    "    \n",
    "    # Create multi-level column structure\n",
    "    methods = sorted(set(key.split('_')[0] for key in data_dict.keys()))\n",
    "    \n",
    "    # Build header rows\n",
    "    header1 = []\n",
    "    header2 = []\n",
    "    for method in methods:\n",
    "        header1.extend([method, \"\", \"\"])  # Superheader spans 3 columns\n",
    "        header2.extend([\"Size 1\", \"Size 2\", \"Size 3\"])  # Subheaders\n",
    "    \n",
    "    # Find maximum length of arrays\n",
    "    max_len = max(len(v) for v in data_dict.values())\n",
    "    \n",
    "    # Build data rows with NaN padding for shorter arrays\n",
    "    data_rows = []\n",
    "    for i in range(max_len):\n",
    "        row = []\n",
    "        for method in methods:\n",
    "            for size in ['size1', 'size2', 'size3']:\n",
    "                key = f\"{method}_{size}\"\n",
    "                value = data_dict[key][i] if i < len(data_dict[key]) else np.nan\n",
    "                row.append(value)\n",
    "        data_rows.append(row)\n",
    "    \n",
    "    # Write to CSV with two header rows\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write superheaders\n",
    "        f.write(','.join(f'\"{h}\"' for h in header1) + '\\n')\n",
    "        # Write subheaders\n",
    "        f.write(','.join(f'\"{h}\"' for h in header2) + '\\n')\n",
    "        # Write data\n",
    "        for row in data_rows:\n",
    "            f.write(','.join(str(x) if not np.isnan(x) else '' for x in row) + '\\n')\n",
    "\n",
    "def safe_to_dataframe(data_dict):\n",
    "    \"\"\"Convert dictionary to DataFrame, handling unequal lengths\"\"\"\n",
    "    if not data_dict:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    max_len = max(len(v) for v in data_dict.values())\n",
    "    padded = {k: np.pad(v, (0, max_len - len(v)), \n",
    "             mode='constant', constant_values=np.nan)\n",
    "             for k, v in data_dict.items()}\n",
    "    return pd.DataFrame(padded)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open('df_list_Corona_Hack.pkl', 'rb') as f:\n",
    "            df_list = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input file not found\")\n",
    "        return\n",
    "    except pickle.PickleError:\n",
    "        print(\"Error: Could not unpickle the file\")\n",
    "        return\n",
    "\n",
    "    setting_names = {\n",
    "        (False, False): \"NonPrivateModel_NonPrivateConformal\",\n",
    "        (False, True): \"NonPrivateModel_PrivateConformal\",\n",
    "        (True, False): \"PrivateModel_NonPrivateConformal\",\n",
    "        (True, True): \"PrivateModel_PrivateConformal\"\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        'coverage': {setting: {} for setting in setting_names.values()},\n",
    "        'size_proportions': {setting: {} for setting in setting_names.values()}\n",
    "    }\n",
    "\n",
    "    for df_idx, df in enumerate(df_list):\n",
    "        try:\n",
    "            private_model = df[\"PrivateModel\"].iloc[0]\n",
    "            private_conformal = df[\"PrivateConformal\"].iloc[0]\n",
    "            setting = setting_names[(private_model, private_conformal)]\n",
    "            \n",
    "            # Coverage data - handle unequal lengths\n",
    "            for method in [\"NonprivQuant\", \"Anas et. al\", \"PrivQuant\"]:\n",
    "                if method in df.columns:\n",
    "                    cov_data = df[method].dropna().astype(float).values\n",
    "                    results['coverage'][setting][method] = cov_data\n",
    "            \n",
    "            # Size proportions\n",
    "            for method in [\"NonprivQuant\", \"Anas et. al\", \"PrivQuant\"]:\n",
    "                size_key = f\"sizes_{method}\"\n",
    "                if size_key in df.columns:\n",
    "                    size_props = compute_size_proportions(df[size_key])\n",
    "                    if len(size_props['size1']) > 0:\n",
    "                        results['size_proportions'][setting].update({\n",
    "                            f\"{method}_size1\": size_props['size1'],\n",
    "                            f\"{method}_size2\": size_props['size2'],\n",
    "                            f\"{method}_size3\": size_props['size3']\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataframe #{df_idx+1}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save coverage data with padding for unequal lengths\n",
    "    for setting in setting_names.values():\n",
    "        if results['coverage'][setting]:\n",
    "            df = safe_to_dataframe(results['coverage'][setting])\n",
    "            df.to_csv(f'coverage_{setting}.csv', index=False)\n",
    "\n",
    "    # Save size proportion data with superheaders\n",
    "    for setting in setting_names.values():\n",
    "        if results['size_proportions'][setting]:\n",
    "            save_size_proportions_to_csv(\n",
    "                results['size_proportions'][setting],\n",
    "                f'size_proportions_{setting}.csv'\n",
    "            )\n",
    "\n",
    "    print(\"Processing complete! Files saved:\")\n",
    "    print(\"- coverage_[setting].csv\")\n",
    "    print(\"- size_proportions_[setting].csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f07be-3a37-4ac7-b09b-50d69494741f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
